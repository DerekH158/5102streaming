{"cells":[{"cell_type":"markdown","source":["##### Course: Data Gathering & Warehousing\n##### Instructor: Carl Chatterton \n##### Lab: #2\n---\n\n### Part 2 - Stream Processing \n\n##### Learning Objectives:\n- Learn how to set up a Spark Structured Streaming pipeline\n- Learn how to consume data from websockets using the Alpaca SDK or request library in python\n- Learn how to develop a schema for data in a DataLake (Amazon S3) \n- Use the `bronze, silver, gold pattern` to prepare aggregate tables using Delta Lake and Spark Structured Streaming\n\n##### Setup Instructions:\n1. Create a Free (Paper Trading) Account at [Alpaca Markets](https://alpaca.markets/)\n2. Store the API credentials as environment variables in Databricks in the Spark cluster configs\n3. Install the alpaca-trade-api library from pypi\n3. Run this notebook to stream live Crypto market data (from Coinbase) to S3 as JSON\n4. Create a new notebook to called `lab-2-stream-processing-<YOUR NAME>` for your submission, this new notebook will be your ELT script to complete the pattern by using Structured Streaming and Delta to process `bronze -> silver -> gold`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"09e28735-bbba-423d-a817-6b46394acedc"}}},{"cell_type":"code","source":["import json\nimport time\nfrom os import getenv\nimport asyncio\nfrom pyspark.sql import Row, SparkSession\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import expr\nfrom alpaca_trade_api.stream import Stream\nfrom delta.tables import DeltaTable\nimport nest_asyncio\n\nnest_asyncio.apply() # This line is needed when working with Juypter Notebooks "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Import Libraries and Dependencies","showTitle":true,"inputWidgets":{},"nuid":"70b11d7c-35fc-4d73-8c69-14d58bb2225b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark = SparkSession.builder.appName('DSSA').getOrCreate()\nspark.getActiveSession()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Get or Create a New Spark Session","showTitle":true,"inputWidgets":{},"nuid":"a6780f61-c05d-444d-b786-788477b687fa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=3473015989495363#setting/sparkui/1202-231301-k2xpz061/driver-867013827682980953\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=3473015989495363#setting/sparkui/1202-231301-k2xpz061/driver-867013827682980953\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["alpaca_api_key = getenv(\"APCA_API_KEY_ID\")\nalpaca_secret_key = getenv(\"APCA_API_SECRET_KEY\")\nalpaca_base_url = getenv(\"APCA_API_BASE_URL\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Load Environment Variables Containing Alpaca API Keys","showTitle":true,"inputWidgets":{},"nuid":"aae765ef-e517-4680-976a-c82eed965d43"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Function that tries to repair connections to the websocket should a connection error occur\ndef run_connection(conn):\n  # Make sure we have an event loop, if not create one\n  loop = asyncio.get_event_loop()\n  loop.set_debug(True)\n  \n  try:\n      conn.run()\n  except KeyboardInterrupt:\n      print(\"Interrupted execution by user\")\n      loop.run_until_complete(conn.stop_ws())\n      exit(0)\n  except Exception as e:\n      print(f'Exception from websocket connection: {e}')\n  finally:\n      print(\"Trying to re-establish connection\")\n      time.sleep(3)\n      run_connection(conn)\n\nasync def on_crypto_bar(bar):\n  # Ignore Bars data that is not exchanged on Coinbase\n  if bar.exchange != 'CBSE':\n      return\n  # Extract the values from the Alpaca responce class and add them to a new dict\n  values = bar.__dict__.pop('_raw')\n  bars = dict()\n  bars['symbol'] = values.pop('symbol')\n  bars['timestamp'] = values.pop('timestamp')\n  bars['exchange'] = values.pop('exchange')\n  bars['open'] = values.pop('open')\n  bars['high'] = values.pop('high')\n  bars['low'] = values.pop('low')\n  bars['close'] = values.pop('close')\n  bars['volume'] = values.pop('volume')\n  bars['trade_count'] = values.pop('trade_count')\n  bars['vwap'] = values.pop('vwap')\n  \n  df = spark.read.json(sc.parallelize([bars]))\n  df.coalesce(1).write.mode('append').json('alpaca/crypto/btc/bars')\n  #print(dbutils.fs.ls('./alpaca/crypto/btc/bars'))\n\nif __name__ == '__main__':\n    conn = Stream(alpaca_api_key, \n                  alpaca_secret_key,\n                  data_feed='iex')\n\n    conn.subscribe_crypto_bars(on_crypto_bar, \"BTCUSD\")\n\n    run_connection(conn)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Create a Web Socket \"Listener\" that is Subscribed to Receive Bitcoin Bar Updates with Delta as a Sink","showTitle":true,"inputWidgets":{},"nuid":"fafc3910-c5c0-4713-962d-8e7452dfe204"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"data websocket error, restarting connection: code = 1006 (connection closed abnormally [internal]), no reason\ndata websocket error, restarting connection: code = 1006 (connection closed abnormally [internal]), no reason\ndata websocket error, restarting connection: code = 1006 (connection closed abnormally [internal]), no reason\nerror during websocket communication: An error occurred while calling o2508.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 330.0 failed 1 times, most recent failure: Lost task 0.0 in stage 330.0 (TID 14960) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 218, in _run_forever\n    await self._consume()\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 100, in _consume\n    await self._dispatch(msg)\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 150, in _dispatch\n    await handler(self._cast(msg_type, msg))\n  File \"<command-2123697561376664>\", line 39, in on_crypto_bar\n    df.coalesce(1).write.mode('append').json('alpaca/crypto/btc/bars')\n  File \"/databricks/spark/python/pyspark/sql/readwriter.py\", line 846, in json\n    self._jwrite.json(path)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n    return_value = get_return_value(\n  File \"/databricks/spark/python/pyspark/sql/utils.py\", line 117, in deco\n    return f(*a, **kw)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o2508.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 330.0 failed 1 times, most recent failure: Lost task 0.0 in stage 330.0 (TID 14960) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nerror during websocket communication: An error occurred while calling o2536.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 332.0 failed 1 times, most recent failure: Lost task 0.0 in stage 332.0 (TID 14969) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 218, in _run_forever\n    await self._consume()\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 100, in _consume\n    await self._dispatch(msg)\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 150, in _dispatch\n    await handler(self._cast(msg_type, msg))\n  File \"<command-2123697561376664>\", line 39, in on_crypto_bar\n    df.coalesce(1).write.mode('append').json('alpaca/crypto/btc/bars')\n  File \"/databricks/spark/python/pyspark/sql/readwriter.py\", line 846, in json\n    self._jwrite.json(path)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n    return_value = get_return_value(\n  File \"/databricks/spark/python/pyspark/sql/utils.py\", line 117, in deco\n    return f(*a, **kw)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o2536.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 332.0 failed 1 times, most recent failure: Lost task 0.0 in stage 332.0 (TID 14969) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nerror during websocket communication: An error occurred while calling o2562.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 334.0 failed 1 times, most recent failure: Lost task 0.0 in stage 334.0 (TID 14978) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 218, in _run_forever\n    await self._consume()\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 100, in _consume\n    await self._dispatch(msg)\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 150, in _dispatch\n    await handler(self._cast(msg_type, msg))\n  File \"<command-2123697561376664>\", line 39, in on_crypto_bar\n    df.coalesce(1).write.mode('append').json('alpaca/crypto/btc/bars')\n  File \"/databricks/spark/python/pyspark/sql/readwriter.py\", line 846, in json\n    self._jwrite.json(path)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n    return_value = get_return_value(\n  File \"/databricks/spark/python/pyspark/sql/utils.py\", line 117, in deco\n    return f(*a, **kw)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o2562.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 334.0 failed 1 times, most recent failure: Lost task 0.0 in stage 334.0 (TID 14978) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nerror during websocket communication: An error occurred while calling o2588.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 336.0 failed 1 times, most recent failure: Lost task 0.0 in stage 336.0 (TID 14987) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 218, in _run_forever\n    await self._consume()\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 100, in _consume\n    await self._dispatch(msg)\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 150, in _dispatch\n    await handler(self._cast(msg_type, msg))\n  File \"<command-2123697561376664>\", line 39, in on_crypto_bar\n    df.coalesce(1).write.mode('append').json('alpaca/crypto/btc/bars')\n  File \"/databricks/spark/python/pyspark/sql/readwriter.py\", line 846, in json\n    self._jwrite.json(path)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n    return_value = get_return_value(\n  File \"/databricks/spark/python/pyspark/sql/utils.py\", line 117, in deco\n    return f(*a, **kw)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o2588.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 336.0 failed 1 times, most recent failure: Lost task 0.0 in stage 336.0 (TID 14987) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nerror during websocket communication: An error occurred while calling o2614.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 338.0 failed 1 times, most recent failure: Lost task 0.0 in stage 338.0 (TID 14996) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 218, in _run_forever\n    await self._consume()\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 100, in _consume\n    await self._dispatch(msg)\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 150, in _dispatch\n    await handler(self._cast(msg_type, msg))\n  File \"<command-2123697561376664>\", line 39, in on_crypto_bar\n    df.coalesce(1).write.mode('append').json('alpaca/crypto/btc/bars')\n  File \"/databricks/spark/python/pyspark/sql/readwriter.py\", line 846, in json\n    self._jwrite.json(path)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n    return_value = get_return_value(\n  File \"/databricks/spark/python/pyspark/sql/utils.py\", line 117, in deco\n    return f(*a, **kw)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o2614.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 338.0 failed 1 times, most recent failure: Lost task 0.0 in stage 338.0 (TID 14996) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nerror during websocket communication: An error occurred while calling o2640.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 340.0 failed 1 times, most recent failure: Lost task 0.0 in stage 340.0 (TID 15005) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 218, in _run_forever\n    await self._consume()\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 100, in _consume\n    await self._dispatch(msg)\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 150, in _dispatch\n    await handler(self._cast(msg_type, msg))\n  File \"<command-2123697561376664>\", line 39, in on_crypto_bar\n    df.coalesce(1).write.mode('append').json('alpaca/crypto/btc/bars')\n  File \"/databricks/spark/python/pyspark/sql/readwriter.py\", line 846, in json\n    self._jwrite.json(path)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n    return_value = get_return_value(\n  File \"/databricks/spark/python/pyspark/sql/utils.py\", line 117, in deco\n    return f(*a, **kw)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o2640.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 340.0 failed 1 times, most recent failure: Lost task 0.0 in stage 340.0 (TID 15005) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nerror during websocket communication: An error occurred while calling o2666.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 342.0 failed 1 times, most recent failure: Lost task 0.0 in stage 342.0 (TID 15014) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 218, in _run_forever\n    await self._consume()\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 100, in _consume\n    await self._dispatch(msg)\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 150, in _dispatch\n    await handler(self._cast(msg_type, msg))\n  File \"<command-2123697561376664>\", line 39, in on_crypto_bar\n    df.coalesce(1).write.mode('append').json('alpaca/crypto/btc/bars')\n  File \"/databricks/spark/python/pyspark/sql/readwriter.py\", line 846, in json\n    self._jwrite.json(path)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n    return_value = get_return_value(\n  File \"/databricks/spark/python/pyspark/sql/utils.py\", line 117, in deco\n    return f(*a, **kw)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o2666.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 342.0 failed 1 times, most recent failure: Lost task 0.0 in stage 342.0 (TID 15014) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nerror during websocket communication: An error occurred while calling o2692.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 344.0 failed 1 times, most recent failure: Lost task 0.0 in stage 344.0 (TID 15023) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 218, in _run_forever\n    await self._consume()\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 100, in _consume\n    await self._dispatch(msg)\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 150, in _dispatch\n    await handler(self._cast(msg_type, msg))\n  File \"<command-2123697561376664>\", line 39, in on_crypto_bar\n    df.coalesce(1).write.mode('append').json('alpaca/crypto/btc/bars')\n  File \"/databricks/spark/python/pyspark/sql/readwriter.py\", line 846, in json\n    self._jwrite.json(path)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n    return_value = get_return_value(\n  File \"/databricks/spark/python/pyspark/sql/utils.py\", line 117, in deco\n    return f(*a, **kw)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o2692.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 344.0 failed 1 times, most recent failure: Lost task 0.0 in stage 344.0 (TID 15023) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["data websocket error, restarting connection: code = 1006 (connection closed abnormally [internal]), no reason\ndata websocket error, restarting connection: code = 1006 (connection closed abnormally [internal]), no reason\ndata websocket error, restarting connection: code = 1006 (connection closed abnormally [internal]), no reason\nerror during websocket communication: An error occurred while calling o2508.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 330.0 failed 1 times, most recent failure: Lost task 0.0 in stage 330.0 (TID 14960) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 218, in _run_forever\n    await self._consume()\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 100, in _consume\n    await self._dispatch(msg)\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 150, in _dispatch\n    await handler(self._cast(msg_type, msg))\n  File \"<command-2123697561376664>\", line 39, in on_crypto_bar\n    df.coalesce(1).write.mode('append').json('alpaca/crypto/btc/bars')\n  File \"/databricks/spark/python/pyspark/sql/readwriter.py\", line 846, in json\n    self._jwrite.json(path)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n    return_value = get_return_value(\n  File \"/databricks/spark/python/pyspark/sql/utils.py\", line 117, in deco\n    return f(*a, **kw)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o2508.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 330.0 failed 1 times, most recent failure: Lost task 0.0 in stage 330.0 (TID 14960) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nerror during websocket communication: An error occurred while calling o2536.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 332.0 failed 1 times, most recent failure: Lost task 0.0 in stage 332.0 (TID 14969) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 218, in _run_forever\n    await self._consume()\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 100, in _consume\n    await self._dispatch(msg)\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 150, in _dispatch\n    await handler(self._cast(msg_type, msg))\n  File \"<command-2123697561376664>\", line 39, in on_crypto_bar\n    df.coalesce(1).write.mode('append').json('alpaca/crypto/btc/bars')\n  File \"/databricks/spark/python/pyspark/sql/readwriter.py\", line 846, in json\n    self._jwrite.json(path)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n    return_value = get_return_value(\n  File \"/databricks/spark/python/pyspark/sql/utils.py\", line 117, in deco\n    return f(*a, **kw)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o2536.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 332.0 failed 1 times, most recent failure: Lost task 0.0 in stage 332.0 (TID 14969) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nerror during websocket communication: An error occurred while calling o2562.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 334.0 failed 1 times, most recent failure: Lost task 0.0 in stage 334.0 (TID 14978) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 218, in _run_forever\n    await self._consume()\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 100, in _consume\n    await self._dispatch(msg)\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 150, in _dispatch\n    await handler(self._cast(msg_type, msg))\n  File \"<command-2123697561376664>\", line 39, in on_crypto_bar\n    df.coalesce(1).write.mode('append').json('alpaca/crypto/btc/bars')\n  File \"/databricks/spark/python/pyspark/sql/readwriter.py\", line 846, in json\n    self._jwrite.json(path)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n    return_value = get_return_value(\n  File \"/databricks/spark/python/pyspark/sql/utils.py\", line 117, in deco\n    return f(*a, **kw)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o2562.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 334.0 failed 1 times, most recent failure: Lost task 0.0 in stage 334.0 (TID 14978) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nerror during websocket communication: An error occurred while calling o2588.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 336.0 failed 1 times, most recent failure: Lost task 0.0 in stage 336.0 (TID 14987) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 218, in _run_forever\n    await self._consume()\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 100, in _consume\n    await self._dispatch(msg)\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 150, in _dispatch\n    await handler(self._cast(msg_type, msg))\n  File \"<command-2123697561376664>\", line 39, in on_crypto_bar\n    df.coalesce(1).write.mode('append').json('alpaca/crypto/btc/bars')\n  File \"/databricks/spark/python/pyspark/sql/readwriter.py\", line 846, in json\n    self._jwrite.json(path)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n    return_value = get_return_value(\n  File \"/databricks/spark/python/pyspark/sql/utils.py\", line 117, in deco\n    return f(*a, **kw)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o2588.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 336.0 failed 1 times, most recent failure: Lost task 0.0 in stage 336.0 (TID 14987) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nerror during websocket communication: An error occurred while calling o2614.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 338.0 failed 1 times, most recent failure: Lost task 0.0 in stage 338.0 (TID 14996) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 218, in _run_forever\n    await self._consume()\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 100, in _consume\n    await self._dispatch(msg)\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 150, in _dispatch\n    await handler(self._cast(msg_type, msg))\n  File \"<command-2123697561376664>\", line 39, in on_crypto_bar\n    df.coalesce(1).write.mode('append').json('alpaca/crypto/btc/bars')\n  File \"/databricks/spark/python/pyspark/sql/readwriter.py\", line 846, in json\n    self._jwrite.json(path)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n    return_value = get_return_value(\n  File \"/databricks/spark/python/pyspark/sql/utils.py\", line 117, in deco\n    return f(*a, **kw)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o2614.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 338.0 failed 1 times, most recent failure: Lost task 0.0 in stage 338.0 (TID 14996) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nerror during websocket communication: An error occurred while calling o2640.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 340.0 failed 1 times, most recent failure: Lost task 0.0 in stage 340.0 (TID 15005) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 218, in _run_forever\n    await self._consume()\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 100, in _consume\n    await self._dispatch(msg)\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 150, in _dispatch\n    await handler(self._cast(msg_type, msg))\n  File \"<command-2123697561376664>\", line 39, in on_crypto_bar\n    df.coalesce(1).write.mode('append').json('alpaca/crypto/btc/bars')\n  File \"/databricks/spark/python/pyspark/sql/readwriter.py\", line 846, in json\n    self._jwrite.json(path)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n    return_value = get_return_value(\n  File \"/databricks/spark/python/pyspark/sql/utils.py\", line 117, in deco\n    return f(*a, **kw)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o2640.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 340.0 failed 1 times, most recent failure: Lost task 0.0 in stage 340.0 (TID 15005) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nerror during websocket communication: An error occurred while calling o2666.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 342.0 failed 1 times, most recent failure: Lost task 0.0 in stage 342.0 (TID 15014) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 218, in _run_forever\n    await self._consume()\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 100, in _consume\n    await self._dispatch(msg)\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 150, in _dispatch\n    await handler(self._cast(msg_type, msg))\n  File \"<command-2123697561376664>\", line 39, in on_crypto_bar\n    df.coalesce(1).write.mode('append').json('alpaca/crypto/btc/bars')\n  File \"/databricks/spark/python/pyspark/sql/readwriter.py\", line 846, in json\n    self._jwrite.json(path)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n    return_value = get_return_value(\n  File \"/databricks/spark/python/pyspark/sql/utils.py\", line 117, in deco\n    return f(*a, **kw)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o2666.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 342.0 failed 1 times, most recent failure: Lost task 0.0 in stage 342.0 (TID 15014) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nerror during websocket communication: An error occurred while calling o2692.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 344.0 failed 1 times, most recent failure: Lost task 0.0 in stage 344.0 (TID 15023) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 218, in _run_forever\n    await self._consume()\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 100, in _consume\n    await self._dispatch(msg)\n  File \"/databricks/python/lib/python3.8/site-packages/alpaca_trade_api/stream.py\", line 150, in _dispatch\n    await handler(self._cast(msg_type, msg))\n  File \"<command-2123697561376664>\", line 39, in on_crypto_bar\n    df.coalesce(1).write.mode('append').json('alpaca/crypto/btc/bars')\n  File \"/databricks/spark/python/pyspark/sql/readwriter.py\", line 846, in json\n    self._jwrite.json(path)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n    return_value = get_return_value(\n  File \"/databricks/spark/python/pyspark/sql/utils.py\", line 117, in deco\n    return f(*a, **kw)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o2692.json.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:560)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:485)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:461)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:153)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:938)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:412)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:853)\n\tat sun.reflect.GeneratedMethodAccessor723.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 344.0 failed 1 times, most recent failure: Lost task 0.0 in stage 344.0 (TID 15023) (ip-10-172-225-162.us-west-2.compute.internal executor driver): java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1334)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3181)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3110)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1096)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2490)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:289)\n\t... 43 more\nCaused by: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 10001)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:359)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:350)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:342)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:341)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:562)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:560)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:155)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:561)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:51)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:266)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:261)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:258)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:305)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:297)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123)\n\t\tat com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:155)\n\t\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:145)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:373)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:300)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\t\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\t\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1621)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t\t... 1 more\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#dbutils.fs.rm('alpaca/crypto/btc/bars', True) # Removes Existing Bronze Data\n#dbutils.fs.rm('/alpaca/crypto/btc/silver_bars/', True) # Removes Existing Silver Data\n# dbutils.fs.rm('/alpaca/crypto/btc/gold_bars/', True) # Remove Existing Gold Data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef12d5a2-bf25-488e-a027-bc0d19739032"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Lab-2-Websocket-Listener","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2123697561376659}},"nbformat":4,"nbformat_minor":0}
